\chapter{RDF data generation from non-RDF data}
\gh{I would avoid the word "engine" in this chapter: an engine is typically a specific part/component of a solution or implementation. So maybe go for "implementation".}
\TODO{Try to replace "engine" with "implementation" where possible.}

\gh{In this paragraph, I would first say that there are specifications and languages that say how to generate RDF.
Then say that there are implementations of these specs, etc.}
How do we generate RDF data from non-RDF data? A variety of specifications and languages exists
which define the generation of RDF data from non-RDF data. These specifications and languages can be grouped 
into two major groups; query languages and mapping languages. Several state-of-the art implementations, 
utilizing these two groups of languages, exist to generate RDF data from non-RDF data.

The implementations could be categorized into two groups; \emph{query-based} and 
\emph{rule-based} implementations. These can be further categorized into two more subgroups 
based on the data that they consume; \emph{bounded} and \emph{unbounded} data. As mentioned in Chapter~\ref{chap:intro}, this work will focus on 
implementations working with unbounded data in a streaming environment, thus implementations 
consuming bounded data will not be elaborated. Before we dive into 
the details of the implementations, we need to elaborate more on the \emph{languages} these implementations 
employ to transform non-RDF to RDF data. 


\section{SPARQL Query Language}
\gh{I see why you put a section about SPARQL here, but actually it belongs to the "Semantic Web" chapter. It's even in Figure 2.1 :) 
Also because RDF-Gen has little to do with SPARQL.}
\TODO{I still feel that it should be here, but if it's moved to "Semantic Web" chapter, 
wouldn't it be duplicate if I mention it here again?}
Query languages such as SQL~\cite{sql} already exist in established relational database systems such as 
MySQL or PostgreSQL. It allows users to manipulate and retrieve data 
from relational databases using concise statements. Due to its widespread 
use in the industry for querying databases, it is important that a similar 
query language is used for RDF datasets to ease the transition for the users. 

SPARQL~\cite{sparql} achieves this goal by emulating  SQL to allow 
a seamless transition to RDF by existing data engineers. 
Similar to relational databases, RDF 
datasets can be considered as a table consisting of three columns --- the \textit{subject} column, 
the \textit{predicate} column, and the \textit{object} column. Unlike relational databases, 
RDF datasets, in a table representation, allow the object column to be of heterogeneous datatype. 
Recall from Section~\ref{sec:turtle_syntax}, one could explicitly specify the 
datatype of the object term which allows the heterogeneity in the object column.


Also, different from SQL, SPARQL allows matching based on \emph{basic graph patterns} composed 
of a set of \emph{triple patterns}. Triple patterns are similar to the triple statements 
as clarified in Section~\ref{sec:turtle_syntax} but extended with declared variables (i.e. \emph{?variable\_name}). 
The declared variables are then bound to the concrete value in the corresponding \emph{triple statements},
matching the given \emph{triple pattern}, from the RDF dataset. 
The result of a SPARQL query is returned as an RDF sub-graph of the queried RDF dataset. 

\gh{If this section (4.1) moves to chapter 2 "Semantic Web Technologies", this paragraph can be deleted. But keep the example (listing 4.1 table 4.1).}
Now a question definitely gets raised in our mind, how does this relate to transforming an 
unbounded dataset to RDF format in a streaming environment? There exists state-of-the-art 
engines for generating RDF data from heterogeneous streaming data sources, which will be 
elaborated in Section~\ref{sec:query_based_implementations}. 

\begin{lstlisting}[language=SPARQL,
    caption={Example of a SPARQL query of a medication.}, 
    label={lst:sparql_example}]
    SELECT ?medication
    WHERE {
      #Basic graph pattern consisting of 2 triple patterns. 
        ?diagnosis example:name "Cancer" .
        ?medication example:canTreat ?diagnosis .
    }
\end{lstlisting}


\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|}
    \hline
    \textbf{medication}        \\ \hline
    Radiation therapy \\ \hline
    \end{tabular}
    \caption{Result of executing the SPARQL query in Listing~\ref{lst:sparql_example}}
    \label{tab:sparql_result}
\end{table}


\section{RDF Mapping Language}
RDF Mapping Language~\cite{rml} is a superset of the W3C's R2RML~\cite{r2rml} which maps relational databases to
RDF datasets. RML improves upon R2RML by expressing mapping rules from heterogeneous
data sources and transforming them to RDF datasets whereas R2RML could only consume
data from relational databases. An RML mapping document is composed of one or more \emph{triples maps}, 
which in turn consist of \emph{subject, predicate} and \emph{object} term maps. As the names imply, 
the term maps are used to map elements of the data sources to their respective terms 
in an RDF triple. The definitions of these maps are similar to the 
specifications in R2RML~\cite{rml_tech}. 

Logical sources could be defined by specifying the \emph{source, logical iterator} 
and zero or one \emph{reference formulation} property.
Although the reference implementation RMLMapper could only process logical sources 
consisting of bounded data, the expressiveness and the flexibility of RML allows one 
to extend it to support unbounded data sources. For example, RMLStreamer~\cite{rml_streamer} implementation in 
Section~\ref{sec:rml_streamer} extended the base RML vocabulary to support logical sources with unbounded data. 

RML also supports defining relationships amongst the different 
triples maps through the use of \textit{rr:parentTriplesMap, rr:joinCondition, rr:child and rr:parent}
properties. Different triples maps might come from separate logical sources, therefore, 
referencing across triples maps might require applying the join operator across multiple 
logical sources. 


\begin{lstlisting}[caption={An example of an RML mapping document~\cite{rml_tech}}.]
@prefix rr: <http://www.w3.org/ns/r2rml#>.
@prefix rml: <http://semweb.mmlab.be/ns/rml#>.
@prefix ex: <http://example.com/ns#>.
@prefix ql: <http://semweb.mmlab.be/ns/ql#>.
@prefix xsd: <http://www.w3.org/2001/XMLSchema#>.
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@base <http://example.com/ns#>.

<#TransportMapping>
  rml:logicalSource [
    rml:source "Transport.xml" ;
    rml:iterator "/transport/bus/route/stop";
    rml:referenceFormulation ql:XPath;
  ];

  rr:subjectMap [
    rr:template
      "http://trans.example.com/stop/{@id}";
    rr:class ex:Stop
  ];

  rr:predicateObjectMap [
    rr:predicate rdfs:label;
    rr:objectMap [
      rml:reference "."
    ]
  ].
    
\end{lstlisting}

\section{Query based implementations}
\label{sec:query_based_implementations}

When we think of interacting with a data source, querying the data source with a 
query language seems natural since that is the common method to interact 
with a database. Allowing users to query the sources without explicitly 
defining the mapping semantics hides the mapping or data transformation details from the user. 
Moreover, it eases the transition to developing applications using RDF data since the syntaxes are 
similar to existing query languages.
\gh{I don't entirely get the last sentence: why would applications *using* RDF care how the RDF is generated?} 
\TODO{It was just meant to highlight the ease of transition for existing application to start 
using RDF data since you can just build upon the existing query language}
To the best of our knowledge, there exist two state-of-the-art query based implementations for transforming 
unbounded non-RDF data to RDF data: SPARQL-Generate and RDF-Gen.
These engines will be elaborated more in the following sections.


\gh{I think both SPARQL-Generate and RDF-Gen can be called "template based"
solutions. They both define somehow a template where variables of the
original data source can be bound.
These are not to be confused with what is (arguably incorrectly) called
OBDA systems, where they literally take a SPARQL query, translate that to
a query the underlying stream processing framework understands, and 
return the results. If it is a CONSTRUCT query then the output are
triples. Examples: Optique (\url{https://link.springer.com/chapter/10.1007\%2F978-3-642-41242-4_11}) 
and Morph-streams (\url{http://dx.doi.org/10.4018/jswis.2012010103})}


\subsection{SPARQL-Generate}
SPARQL-Generate~\cite{sparql_generate} was proposed as an alternative to then existing methods of 
transforming non-RDF data. The language is based on an extension of SPARQL 1.1 query language, to leverage 
its expressiveness and extensibility. Furthermore, this allows SPARQL-Generate to be implemented on top 
of any existing SPARQL query engine. The reference implementation in the paper was based on Apache Jena's 
SPARQL 1.1 engine. Due to the use of the existing SPARQL 1.1 query language, experienced knowledge engineers could use 
SPARQL-Generate to improve the generation of RDF data in their existing workflow. 

SPARQL-Generate supports the consumption of heterogeneous data sources by exposing the 
\emph{binding} and \emph{iterator functions API}. Therefore, covering a new data format or data source could be accomplished 
by implementing the corresponding \emph{binding} and \emph{iterator functions}. Currently, as of writing this paper, 
the reference implementation supports the consumption of data sources which are unbounded and in a streaming environment. 
For example, WebSocket and MQQT are currently supported in the latest version of SPARQL-Generate. 

Although there is a support for data stream processing, the paper did not go into details 
about the application of multi-stream operators like joins when involving multiple streaming 
data sources. Hence, we could assume that SPARQL-Generate either processes the whole dataset in memory or 
keeps a fixed window of data on which multi-stream operators are applied.
\gh{Could you check what exactly it is? \url{https://github.com/sparql-generate/sparql-generate}}
\TODO{I've mailed the lead researcher about it, should get an answer back soon. Read through the code, 
but it seems like they just fed it into the underlying Jena framework for joins....}
Furthermore, the 
authors mentioned that SPARQL-Generate could use the SPARQL 1.1 operators such as \emph{join operators}. 
From this, we could derive that SPARQL-Generate will apply \emph{join} on the 
records by delegating it to the underlying SPARQL engine.
\gh{Probably, but please check it. If you don't find it,
we can just ask the authors. But I don't like assuming 
things if they can be verified.} 
Therefore, there is a lack of  
dynamic windowing schemes to efficiently exploit the characteristics of the streaming data sources.  

\subsection{RDF-Gen}
RDF-Gen~\cite{rdf_gen} is also based on SPARQL-like syntaxes. However, instead of extending SPARQL engines, 
it provides its own architecture as laid out in Figure~\ref{fig:rdf-gen-arch} to meet the demands
of real-time processing. Instead of adopting most of the syntax from SPARQL, RDF-Gen only 
keeps the basic graph pattern section of SPARQL query to reduce the size of the transformation specification. 
Thus, it has the most compact mapping template compared to the other methods mentioned in this paper. 

RDF-Gen consists of three main components: 
\begin{enumerate*}[label=(\alph*)]
  \item Data Connector,
  \item Triple Generator, and
  \item Link Discovery.
\end{enumerate*}

Data Connector allows close-to-source processing, which is not the case for SPARQL-Generate.\gh{Why not?} 
Triple Generator handles the rapid generation of RDF triples by making use of 
template graphs and variable vectors. The Link Discovery component solves the 
problem of link discovery problem which is defined as follows:
Given two data sources $S$ and $T$, the problem is to find the pairs of elements in 
$S \times T$ that are related to each other (e.g. following the predicate of 
\emph{owl:sameAs} property). Since we are concerned with 
multi-stream operators during RDF generation in this paper, link discovery component could be ignored. 


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{fig/rdf-gen-arch.png}
  \caption{Architecture of RDF-Gen~\cite{rdf_gen}}
  \label{fig:rdf-gen-arch}
\end{figure}

\subsubsection{Data Connector}
Data Connector has a similar functionality as the \emph{iterator functions} from SPARQL-Generate.
It consumes the data sources given a configuration setting. The configuration setting is used to specify 
the type of data sources, the \emph{window} for processing the incoming data records and 
also apply functions on the incoming data elements. Data Connector can thus be defined as a 
mapping function as in Definition~\ref{defn:data_connector}. 


\begin{defn}[Data Connector record~\cite{rdf_gen}]
  \label{defn:data_connector}
  Given a set of data sources $D = \{d_1, d_2, \dots, d_n\}$ and  a 
  mapping function $F = \mu_{f}(d_i, e)\; |\; \forall d_i \in D$ with $e$, a data element 
  of a data source $d_i$, and $f$, a filter function. Data Connector 
  generates a record $R = \mu_{f}(d_i, e)$ $\iff$ all the attributes of 
  $e$ satisfy the filter function $f$. By default, the filter function $f$ just returns true. 
\end{defn}

Using the Definition~\ref{defn:data_connector}, we could now also apply an equi-join operator 
on the data sources. Formally, we could generate a new triple 
$R =  \mu_{f_i}(d_i, e_i) \bowtie  \mu_{f_j}(d_j, e_j) $ where $e_i$ and $e_j$ have 
common attributes under the filter functions $f_i$ and $f_j$. 

The processing is done on individual records, leading to 
RDF-Gen treating streaming and archival data sources the same way --- as “streams” 
of records. Due to the record-by-record processing, the framework also has a very low 
memory usage. 

\subsubsection{Triple Generator}
As indicated in Figure~\ref{fig:rdf-gen-arch}, Triple Generator consumes the output records 
of the Data Connectors to convert them into RDF triples. A vector of variables $V$, an RDF 
graph template $G$, akin to the basic graph pattern from SPARQL 1.1, and a 
set of \emph{functions} $F$ can be used to configure the Triple Generators. 

$V$ consists of variables which corresponds to the attributes of the
generated records from the Data Connector. These variables are referenced 
in the graph template $G$, and then used to 
bind to the attribute value of the record provided by the Data Connector. 
In case these variables are the arguments of a function $f \in F$, the function will 
be evaluated and the result appended to the output. Therefore, 
this simple binding of values in a template graph enables Triple Generator to 
generate RDF triples efficiently and have a high scalability. Generally, to 
keep the computational time low, the functions will have to be simple in complexity. 


Next, we shall work on a small example to understand how Triple Generator works. 
Listing~\ref{lst:rdf_gen_example} shows an example of a graph template $G$ provided to 
the Triple Generator to generate RDF triples. In this example, the provided vector 
of variables is $V = [\textrm{?diagnosis\_id}, \textrm{?name} ]$. If the incoming record 
is as shown in Table~\ref{tab:rdf_gen_sample_record}, the specified variables,
\emph{diagnosis\_id} and \emph{name}, will be bound to the values \emph{100} and 
\emph{Cancer} respectively. Afterwards, the functions \emph{makeUri} and \emph{asString} will 
be called with the bounded values as arguments and the generated 
output will be used to generate the RDF triples specified by 
the graph template. 
The final generated set of RDF triples is (e.g. in turtle syntax): 

\begin{lstlisting}
  ... 
  <http://example.com/100>  a example; 
                            example:name  "Cancer". 
  ...
\end{lstlisting}


\begin{table}[!htbp]
  \centering
  \begin{tabular}{l|l}
  \multicolumn{1}{c|}{\textbf{diagnosis\_id}} & \multicolumn{1}{c}{\textbf{name}} \\ \hline
  100                                 & Cancer                    
  \end{tabular}
  \caption{A sample record generated by the Data Connector.}
  \label{tab:rdf_gen_sample_record}
  \end{table}

\begin{lstlisting}[language={SPARQL},
   caption={A simple graph template $G$ with the functions \emph{asString} and \emph{makeUri}.}, 
   label={lst:rdf_gen_example}
  ]
  #BGP for the diagnosis data source
  makeUri(?diagnosis_id)  a example:Diagnosis;
                          example:name asString(?name).
\end{lstlisting}

Consumption of input on a record-by-record basis results in RDF-Gen having to 
handle multi-stream operators with the use of windows. However, the paper did not 
mention in detail about its implementation of windows to handle 
multi-stream operators since it was out of scope. Therefore, we still have no 
notion of how to handle data streams with dynamic characteristics where fixed windows 
size have negative consequences on the quality of the generated RDF data. 


\section{Mapping based implementations}
\label{sec:mapping_based_implementation}
Other than approaching the transformation of non-RDF to RDF from the viewpoint of 
queries, one could also employ mapping languages such as RML. Mapping languages 
such as these are declarative and have minimal cognitive burden composing it compared to the
query based implementations. Due to defining the mapping relationship for each attribute in 
the input source, one does not need to deal with nested queries. Despite being more 
verbose than query languages, composing complex mapping documents results in much more human-readable 
format due to the direct mapping of the attributes. 
The following subsections will elaborate more on the related state-of-the-art engines 
which utilizes mapping languages in a streaming environment.

\gh{Perhaps mention somewhere that mapping languages are declarative, and what their advantages are (if any), like you	did with the query based implementations.}

\subsection{TripleWave}
Albeit the abundance of solutions to combine semantic technologies with stream and event processing 
techniques, there was a lack of engines to disseminate and exchange RDF streams on the Web~\cite{triple_wave}. 
TripleWave~\cite{triple_wave} was conceived to fill this role; to provide the mechanism to publish and spread RDF streams on the Web. 
It extends R2RML, which only allows ingestion of inputs from relational databases, 
to also consume other formats such as JSON or CSV (just like RML, a superset of R2RML). 

TripleWave generates an RDF stream of JSON-LD format which could be consumed by existing RSP engines for further 
processing.
It also supports joining of multiple streams which could be inferred from its implementation of R2RML's \emph{rr:parentTriplesMap} predicate~\cite{triple_wave}. Since the goal of TripleWave 
is to publish RDF streams from RDF and non-RDF data sources, it does not support the 
application of arbitrary functions at its core unlike RDF-Gen and SPARQL-Generate. 
However, as is the case with the aforementioned frameworks and engines, 
it does not support dynamic windowing to handle streams 
with dynamic characteristics. 


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/triple-wave-arch.png}
  \caption{Architecture of TripleWave generating RDF streams from non-RDF and RDF data sources~\cite{triple_wave}. }
  \label{fig:triple-wave-arch}
\end{figure}

\subsection{RMLStreamer}
\label{sec:rml_streamer}
The engines elaborated thus far, scale insufficiently in terms of velocity and 
volume of data streams. Multiple instances of the engines will need to 
be started in order to scale with the higher volume and velocity of 
the input data streams. Since they are not built upon a distributed framework, 
there is also a need for a separate implementation to coordinate the different 
instances in a distributed environment. 

To tackle the aforementioned shortcomings, RMLStreamer~\cite{rml_streamer}
was developed to parallelize the ingestion and mapping process of RDF data generation pipeline. 
It is based on the work of RMLMapper~\cite{rml}, an RDF mapping engine consuming bounded data and 
mapping them to RDF data with the use of RML. Hence, RMLStreamer can also 
process heterogeneous data and generate RDF data. 

Due to the parallelization of the subtasks, the ingestion and the mapping process, these processes
could be spread over different machines for distributed execution. 
This is visualised in 
Figure \ref{fig:rml-parallel-arch}.  However, as with all distributed 
computing, a mechanism to coordinate the different machines is required. 
To fulfill this requirement, RMLStreamer is 
implemented on top of Apache Flink~\cite{flink} framework, a generic distributed stream 
processing framework. Not only does it allow the mapping 
of non-RDF to RDF data, it also guarantees fault-tolerance through the usage of 
\emph{Asynchronous Barrier Snapshots}~\cite{flink_fault_tolerance} and \emph{exactly-once} semantic,
although these are disabled by default. 

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/rml_streamer_arch.png}
  \caption{Parallelization architecture of RMLStreamer for RDF data generation. RMLStreamer parallelizes 
  the ingestion of the data sources and the mapping process of non-RDF to RDF data. }
  \label{fig:rml-parallel-arch}
\end{figure}


\section{Summary}
While SPARQL-Generate~\cite{sparql_generate}, RDF-Gen~\cite{rdf_gen}, and TripleWave~\cite{triple_wave} 
support the join operator for executing a binary join, but scalability is not built in the engines 
to handle multiple streaming 
data sources with high velocity and volume. 
A solution
with built in capabilities to handle high velocity and volume of data, is of interest
to implement the dynamic windowing scheme. 
Hence, RMLStreamer is the best candidate for this work on dynamic window join in RDF generation. 



