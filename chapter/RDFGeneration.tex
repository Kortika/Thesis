\chapter{RDF data generation from non-RDF data}
\gh{I would avoid the word "engine" in this chapter: an engine is typically a specific part/component of a solution or implementation. So maybe go for "implementation".}


\gh{In this paragraph, I would first say that there are specifications and languages that say how to generate RDF.
Then say that there are implementations of these specs, etc.}

Several state-of-the art implementations exist to generate RDF data from 
non-RDF data. These engines could be categorized into two groups; \emph{query-based} and 
\emph{rule-based} engines. These can be further categorized into two more subgroups 
based on the data that they consume; \emph{bounded} and \emph{unbounded} data processing 
engines. As mentioned in Chapter~\ref{chap:intro}, this work will focus on 
engines working with unbounded data in a streaming environment, thus engines 
consuming bounded data will not be elaborated. Before we dive into 
the engines, we need to elaborate more on the \emph{languages} these engines 
employ to transform non-RDF to RDF data. 


\section{SPARQL Query Language}
\gh{I see why you put a section about SPARQL here, but actually it belongs to the "Semantic Web" chapter. It's even in Figure 2.1 :) 
Also because RDF-Gen has little to do with SPARQL.}
Query languages \gh{such as SQL[++]}\footnote{[++] means: add reference} already exist in established relational database systems such as 
MySQL or PostgreSQL. It allows users to manipulate and retrieve data 
from relational databases using concise statements. Due to its widespread 
use in the industry for querying databases, it is important that a similar 
query language is used for RDF datasets to ease the transition for the users. 

SPARQL\cite{sparql} achieves this goal by emulating \sout{as many} SQL \sout{syntaxes as possible} to allow 
a seamless transition to RDF \sout{datasets usage} by existing data engineers. 
Similar to relational databases, RDF 
datasets can be considered as a table consisting of three columns --- the \textit{subject} column, 
the \textit{predicate} column, and the \textit{object} column. Unlike relational databases, 
RDF datasets, in a table representation, allow the object column to be of heterogeneous datatype. 
Recall from \sout{Chapter}\gh{Section}~\ref{sec:turtle_syntax}, one could explicitly specify the 
datatype of the object term which allows the heterogeneity in the object column.


Also, different from SQL, SPARQL allows matching based on \emph{basic graph patterns} composed 
of a set of \emph{triple patterns}. Triple patterns are similar to the triple statements 
as clarified in Chapter~\ref{sec:turtle_syntax} but extended with declared variables (i.e. \emph{?variable\_name}). 
The declared variables are then bound to the concrete value in the corresponding \emph{triple statements},
matching the given \emph{triple pattern}, from the RDF dataset. 
The result of a SPARQL query is returned as an RDF sub-graph of the queried RDF dataset. 

\gh{If this section (4.1) moves to chapter 2 "Semantic Web Technologies", this paragraph can be deleted. But keep the example (listing 4.1 table 4.1).}
Now a question definitely gets raised in our mind, how does this relate to transforming an 
unbounded dataset to RDF format in a streaming environment? There exists state-of-the-art 
engines for generating RDF data from heterogeneous streaming data sources, which will be 
elaborated in Chapter~\ref{sec:query_based_engine}. 

\begin{lstlisting}[language=SPARQL,
    caption={Example of a SPARQL query of a medication.}, 
    label={lst:sparql_example}]
    SELECT ?medication
    WHERE {
      #Basic graph pattern consisting of 2 triple patterns. 
        ?diagnosis example:name "Cancer" .
        ?medication example:canTreat ?diagnosis .
    }
\end{lstlisting}


\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|}
    \hline
    \textbf{medication}        \\ \hline
    Radiation therapy \\ \hline
    \end{tabular}
    \caption{Result of executing the SPARQL query in Listing~\ref{lst:sparql_example}}
    \label{tab:sparql_result}
\end{table}


\section{RDF Mapping Language}
RDF Mapping Language\cite{rml} is a superset of the W3C's R2RML\cite{r2rml} which maps relational databases to
RDF datasets. RML improves upon R2RML by expressing mapping rules from heterogeneous
data sources and transforming them to RDF datasets whereas R2RML could only consume
data from relational databases. An RML mapping \sout{file}\gh{document} is composed of one or more \emph{triples maps}, 
which in turn consist of \emph{subject, predicate} and \emph{object} term maps. As the names imply, 
the term maps are used to map elements of the data sources to their respective terms 
in an RDF triple. The definitions of these maps are similar to the 
specifications in R2RML\cite{rml_tech}. 

Logical sources could be defined by specifying the \emph{source, logical iterator} 
and zero or one \emph{reference formulation} property.
The logical sources in the default 
RML mapping file are bounded data, where the data already exists and has a predetermined
size.
\gh{This is not (entirely) true: RML does not state if a source is bounded or not. At this moment we
somehow have to derive this from what's in rml:source. However,
The RMLMapper, the 
reference implementation, only processes bounded data sources at the moment.}

RML also supports defining relationships amongst the different 
triple maps through the use of \textit{rr:parentTriplesMap, rr:joinCondition, rr:child and rr:parent}
properties. Different triple maps might come from separate logical sources, therefore, 
referencing across these triple maps will require applying the join operator across multiple 
logical sources. 


\begin{lstlisting}[caption={An example of an RML mapping file\cite{rml_tech}}.]
@prefix rr: <http://www.w3.org/ns/r2rml#>.
@prefix rml: <http://semweb.mmlab.be/ns/rml#>.
@prefix ex: <http://example.com/ns#>.
@prefix ql: <http://semweb.mmlab.be/ns/ql#>.
@prefix xsd: <http://www.w3.org/2001/XMLSchema#>.
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@base <http://example.com/ns#>.

<#TransportMapping>
  rml:logicalSource [
    rml:source "Transport.xml" ;
    rml:iterator "/transport/bus/route/stop";
    rml:referenceFormulation ql:XPath;
  ];

  rr:subjectMap [
    rr:template
      "http://trans.example.com/stop/{@id}";
    rr:class ex:Stop
  ];

  rr:predicateObjectMap [
    rr:predicate rdfs:label;
    rr:objectMap [
      rml:reference "."
    ]
  ].
    
\end{lstlisting}

\section{Query based Engines}
\label{sec:query_based_engine}
When we think of interacting with a data source, querying the data source with a 
query language seems natural since that is the common method to interact 
with a traditional database. Allowing users to query the sources without explicitly 
defining the mapping semantics hides the mapping or data transformation details from the user. 
Moreover, it eases the transition to developing applications using RDF data since the syntaxes are 
similar to existing query languages. 

From what we know, there exists two state-of-the-art query based engines for transforming 
unbounded non-RDF data to RDF data; SPARQL-Generate and RDF-Gen, the latter diverging a lot from
the underlying SPARQL syntaxes.  
These engines will be elaborated more in the following sections. 


\subsection{SPARQL-Generate}
SPARQL-Generate~\cite{sparql_generate} was proposed as an alternative to then existing methods of 
transforming non-RDF data. The language is based on an extension of SPARQL 1.1 query language, to leverage 
its expressiveness and extensibility. Furthermore, this allows SPARQL-Generate to be implemented on top 
of any existing SPARQL query engines. The reference implementation in the paper was based on Apache Jena's 
SPARQL 1.1 engine. Due to the use of existing SPARQL 1.1 query language, experienced knowledge engineers could use 
SPARQL-Generate to improve the generation of RDF data in their existing workflow. 

SPARQL-Generate supports the consumption of heterogeneous data sources by exposing the 
\emph{binding} and \emph{iterator functions API}. Therefore, covering a new data format or data source could be accomplished 
by implementing the corresponding \emph{binding} and \emph{iterator functions}. Currently, as of writing this paper, 
the reference implementation supports the consumption of data sources which are unbounded and in a streaming environment. 
For example, WebSocket and MQQT are currently supported in the latest version of SPARQL-Generate. 

Although there is a support for data stream processing, the paper did not go into details 
about the application of multi-stream operators like joins when involving multiple streaming 
data sources. Hence, we could assume that SPARQL-Generate either process the whole dataset in memory or 
keeps a fixed window of data on which multi-stream operators are applied. Furthermore, the 
authors mentioned that SPARQL-Generate could use the SPARQL 1.1 operators such as \emph{join operators}. 
From this, we could derive that SPARQL-Generate will apply \emph{join} on the 
records by delegating it to the underlying SPARQL engine. 
Therefore, there is a lack of  
dynamic windowing schemes to efficiently exploit the characteristics of the streaming data sources.  

\subsection{RDF-Gen}
RDF-Gen\cite{rdf_gen} is also based on SPARQL-like syntaxes. However, instead of extending SPARQL engines, 
it provides its own architecture as laid out in Figure~\ref{fig:rdf-gen-arch} to meet the demand
of real-time processing. Instead of adopting most of the syntaxes from SPARQL, RDF-Gen only 
kept the BGP section of SPARQL query to reduce the size of the transformation specification. 
Thus, it has the most compact mapping template/query compared to the other methods mentioned in this paper. 

RDF-Gen consists of three main components: 
\begin{enumerate*}[label=(\alph*)]
  \item Data Connector,
  \item Triple Generator, and
  \item Link Discovery.
\end{enumerate*}
Data Connector allows close-to-source processing, which is not the case for SPARQL-Generate. 
Triple Generator handles the rapid generation of RDF triples by making use of 
template graphs and variable vectors. The Link Discovery component solves the 
problem of link discovery problem which is defined as follows:
Given two data sources $S$ and $T$, the problem is to find the pairs of elements in 
$S \times T$ that are related to each other (e.g. following the predicate of 
\emph{owl:sameAs} property). Since we are concerned with 
multi-stream operators during RDF generation in this paper, link discovery component could be ignored. 


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{fig/rdf-gen-arch.png}
  \caption{Architecture of RDF-Gen\cite{rdf_gen}}
  \label{fig:rdf-gen-arch}
\end{figure}

\subsubsection{Data Connector}
Data Connector has a similar functionality as the \emph{iterator functions} from SPARQL-Generate.
It consumes the data sources given a configuration setting. Configuration setting is used to specify 
the type of data sources, the \emph{window} for processing the incoming data records and 
also apply functions on the incoming data elements. Data Connector can thus be defined as a 
mapping function as in Definition~\ref{defn:data_connector}. 


\begin{defn}[Data Connector record \cite{rdf_gen}]
  \label{defn:data_connector}
  Given a set of data sources $D = \{d_1, d_2, \dots, d_n\}$ and  a 
  mapping function $F = \mu_{f}(d_i, e)\; |\; \forall d_i \in D$ with $e$, a data element 
  of a data source $d_i$, and $f$, a filter function. Data Connector 
  generates a record $R = \mu_{f}(d_i, e)$ $\iff$ all the attributes of 
  $e$ satisfy the filter function $f$. By default, the filter function $f$ just returns true. 
\end{defn}

Using the Definition~\ref{defn:data_connector}, we could now also apply an equi-join operator 
on the data sources. Formally, we could generate a new triple 
$R =  \mu_{f_i}(d_i, e_i) \bowtie  \mu_{f_j}(d_j, e_j) $ where $e_i$ and $e_j$ have 
common attributes under the filter functions $f_i$ and $f_j$. 

The processing is done on individual records, leading to 
RDF-Gen treating streaming and archival data sources the same way --- as “streams” 
of records. Due to the record-by-record processing, the framework also has a very low 
memory usage. 

\subsubsection{Triple Generator}
As indicated in Figure~\ref{fig:rdf-gen-arch}, Triple Generator consumes the output records 
of the Data Connectors to convert them into RDF triples. A vector of variables $V$, an RDF 
graph template $G$, akin to the basic graph pattern from SPARQL 1.1, and a 
set of \emph{functions} $F$ can be used to configure the Triple Generators. 

$V$ consists of variables which corresponds to the attributes of the
generated records from the Data Connector. These variables are referenced 
in the graph template $G$, and then used to 
bind to the attribute value of the record provided by the Data Connector. 
In case these variables are the arguments of a function $f \in F$, the function will 
be evaluated and the result appended to the output. Therefore, 
this simple binding of values in a template graph enables Triple Generator to 
generate RDF triples efficiently and have a high scalability. Generally, to 
keep the computational time low, the functions will have to be simple in complexity. 


Next, we shall work on a small example to understand how Triple Generator works. 
Listing~\ref{lst:rdf_gen_example} shows an example of a graph template $G$ provided to 
the Triple Generator to generate RDF triples. In this example, the provided vector 
of variables is $V = [\textrm{?diagnosis\_id}, \textrm{?name} ]$. If the incoming record 
is as shown in Table~\ref{tab:rdf_gen_sample_record}, the specified variables,
\emph{diagnosis\_id} and \emph{name}, will be bound to the values \emph{100} and 
\emph{Cancer} respectively. Afterwards, the functions \emph{makeUri} and \emph{asString} will 
be called with the bounded values as arguments and the generated 
output will be used to generate the RDF triples specified by 
the graph template. 
The final generated set of RDF triples is (e.g. in turtle syntax): 

\begin{lstlisting}
  ... 
  <http://example.com/100>  a example; 
                            example:name  "Cancer". 
  ...
\end{lstlisting}


\begin{table}[!htbp]
  \centering
  \begin{tabular}{l|l}
  \multicolumn{1}{c|}{\textbf{diagnosis\_id}} & \multicolumn{1}{c}{\textbf{name}} \\ \hline
  100                                 & Cancer                    
  \end{tabular}
  \caption{A sample record generated by the Data Connector.}
  \label{tab:rdf_gen_sample_record}
  \end{table}

\begin{lstlisting}[language={SPARQL},
   caption={A simple graph template $G$ with the functions \emph{asString} and \emph{makeUri}.}, 
   label={lst:rdf_gen_example}
  ]
  #BGP for the diagnosis data source
  makeUri(?diagnosis_id)  a example:Diagnosis;
                          example:name asString(?name).
\end{lstlisting}

Consumption of input on a record-by-record basis results in RDF-Gen having to 
handle multi-stream operators with the use of windows. However, the paper did not 
mention in detail about its implementation of windows to handle 
multi-stream operators since it was out of scope. Therefore, we still have no 
notion of how to handle data streams with dynamic characteristics where fixed windows 
size have negative consequences on the quality of the generated RDF data. 


\section{Mapping based Engines}
\label{sec:mapping_based_engine}
Other than approaching the transformation of non-RDF to RDF from the viewpoint of 
queries, one could also employ mapping languages such as RML. 
The following subsections will elaborate more on the related state-of-the-art engines 
which utilizes mapping languages in a streaming environment. 

\subsection{TripleWave}
Albeit the abundance of solutions to combine semantic technologies with stream and event processing 
techniques, there was a lack of engines to disseminate and exchange RDF streams on the Web \cite{triple_wave}. 
TripleWave\cite{triple_wave} was conceived to fill this role; to provide the mechanism to publish and spread RDF streams on the Web. 
It extends R2RML, which only allows ingestion of inputs from relational databases, 
to also consume other formats such as JSON or CSV (just like RML, a superset of R2RML). 

TripleWave generates an RDF stream of JSON-LD format which could be consumed by existing RSP engines for further 
processing. It also supports joining of multiple stream which could be inferred from its usage of R2RML's usage 
of \emph{rr:parentTriplesMap} predicate \cite{triple_wave}. Since the goal of TripleWave 
is to publish RDF streams from RDF and non-RDF data sources, it does not support the 
application of arbitrary functions at its core unlike RDF-Gen and SPARQL-Generate. 
However, as is the case with the aforementioned frameworks and engines, 
it does not support dynamic windowing to handle streams 
with dynamic characteristics. 


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/triple-wave-arch.png}
  \caption{Architecture of TripleWave generating RDF streams from non-RDF and RDF data sources\cite{triple_wave}. }
  \label{fig:triple-wave-arch}
\end{figure}

\subsection{RMLStreamer}
The engines elaborated thus far, scale insufficiently in terms of the velocity and 
volume of the data stream. Multiple instances of the engines will need to 
be started in order to scale with the higher volume and velocity of 
the input data streams. Since they are not built upon a distributed framework, 
there is also a need for a separate implementation to coordinate the different 
instances in a distributed environment. 

To tackle the aforementioned shortcoming, RMLStreamer\cite{rml_streamer}
was developed to parallelize the ingestion and mapping process of RDF data generation pipeline. 
It is based on the work of RMLMapper\cite{rml}, an RDF mapping engine consuming bounded data and 
mapping them to RDF data with the use of RML. Hence, RMLStreamer could also 
process heterogeneous data and generate RDF data. 

Parallelization of the subtasks, the ingestion and the mapping process, is achieved by executing 
these processes over multiple machines. This is visualised in 
Figure \ref{fig:rml-parallel-arch}.  However, as with all distributed 
computing, a mechanism to coordinate the different machines is required. 
To fulfill this requirement, RMLStreamer is 
implemented on top of Apache Flink\cite{flink} framework, a generic distributed stream 
processing framework. Not only does it allow the mapping 
of non-RDF to RDF data, it also guarantees fault-tolerance through the usage of 
\emph{Asynchronous Barrier Snapshots}~\cite{flink_fault_tolerance} and \emph{exactly-once} semantic. 


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/rml_streamer_arch.png}
  \caption{Parallelization architecture of RMLStreamer for RDF data generation. RMLStreamer parallelizes 
  the ingestion of the data sources and the mapping process of non-RDF to RDF data. }
  \label{fig:rml-parallel-arch}
\end{figure}


\section{Summary}
While SPARQL-Generate\cite{sparql_generate}, RDF-Gen\cite{rdf_gen}, and TripleWave\cite{triple_wave} 
supports join operator for executing a binary join, scalability is not built in the engines to handle multiple streaming 
sources of data with high velocity and volume. There needs to be separate instances started on different 
machines and the coordination mechanism implemented for these engines to scale. An engine
with built in capabilities to handle high velocity and volume of data is of interest
to implement the dynamic windowing scheme. 
Hence, RMLStreamer is the best candidate for this work on dynamic window join in RDF generation. 



